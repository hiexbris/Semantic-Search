# ðŸ” Semantic Search over Wikipedia and Web

A semantic question-answering system that retrieves the most relevant context from Wikipedia and Google search results, then uses a lightweight LLM to generate **fact-grounded answers with references**. This project emphasizes transparency, trust, and zero hallucination â€” keeping your LLMs up-to-date **without retraining**.

---

## ðŸŒŸ Features

- âœ… **Hybrid Retrieval** using both Sentence Transformers and TF-IDF by implementing Reciprocal Rank Fusion (RRF).
- âœ… **Per-Source Chunking** for transparent answers per page
- âœ… **No Hallucinations** â€” answers come strictly from retrieved data
- âœ… **Low Maintenance** â€” no retraining required to stay current
- âœ… **BLEU and BERTScore Evaluation**
- âœ… **Modular and Extendable Pipeline**

---

## ðŸ“‚ Project Overview

### ðŸ§  Knowledge Construction

- `top_pages.py`  
  Fetches **top 25,000 Wikipedia pages** from the last 5 years based on page views.

- `summary_pages.py`  
  Downloads **summaries** of those Wikipedia pages.

- `embeddings_summary_pages.py`  
  Creates both **dense (Sentence Transformer)** and **sparse (TF-IDF)** embeddings for summaries, stores in a FAISS index.

---

### ðŸ” Query-Time Retrieval

- `top_wiki_pages.py`  
  Retrieves **most relevant Wikipedia pages** for a given query using hybrid search. It combines dense (Sentence Transformers) and sparse (TF-IDF) retrieval using Reciprocal Rank Fusion (RRF).

- `top_non_wiki_pages.py`  
  Downloads **top web pages** from Google using SERPAPI.

- `embedding_entire_page.py`  
  Chunks each full page (~150 words/chunk), computes dense and sparse embeddings for each chunk.

- `hybrid_chunk_retrieval.py`  
  Retrieves top chunks for a given query from both Wikipedia and web sources.

---

### ðŸ¤– Answer Generation

- `LLM_output.py`  
  Sends chunks to the LLM to generate **per-source answers**, ensuring traceable and trustworthy output.

- `main.py`  
  Complete pipeline: query â†’ retrieval â†’ chunking â†’ per-site answer generation with source links.

---

### ðŸ“ˆ Evaluation Tools

- `eval_semantic.py`  
  Generates an answer using retrieved passages.

- `eval_LLM.py`  
  Lets the LLM generate an answer without any retrieved context (pure LLM knowledge).

- `eval.py`  
  Compares generated answers against references using **BLEU** and **BERTScore**.

---

## ðŸ—‚ï¸ Input/Output Format

- `queries.txt`  
  Contains all queries to be processed. Each line is a separate question.

- `answers/`  
  Final answers generated for each query using the full hybrid pipeline.

- `LLM/`  
  Stores LLM-only answers (no retrieval used).

- `semantic/`  
  Stores answers generated from semantic-retrieved passages.

- `GPT/`  
  Reference answers or answers from another baseline (e.g., GPT-4 baseline).

---

## âš™ï¸ Performance Notes

> â±ï¸ Expect slightly longer response times compared to a plain LLM.

This is due to:

- Real-time **webpage downloads**, requiring delay between API calls.
- On-the-fly **embedding computations** for chunked content.

âœ… This can be improved by:

- **Batching embeddings** (require good VRAM)
- Using **premium API keys**
- Implementing caching for frequently queried sites

---

## ðŸ’¡ Why Use This?

âŒ Standard LLMs often hallucinate.  
âŒ They can't cite sources.  
âŒ Keeping them up-to-date requires **expensive** fine-tuning.

âœ… This system keeps your answers grounded in **fresh, real-world data** â€” without any retraining.  
âœ… Ideal for **news bots**, **research assistants**, **academic agents**, and **factual QA systems**.

> "Donâ€™t hallucinate â€” retrieve, reason, and reference."

